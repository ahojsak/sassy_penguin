<html>

<head>
    <link rel="stylesheet" href="http://netdna.bootstrapcdn.com/bootstrap/3.0.3/css/bootstrap.min.css">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Final Project</title>
	<style>

	.chart div {
	  font: 10px sans-serif;
	  background-color: steelblue;
	  text-align: right;
	  padding: 3px;
	  margin: 1px;
	}
	
	.axis text {
	  font: 10px sans-serif;
	}

	.axis path,
	.axis line {
	  fill: none;
	  stroke: #000;
	}
	

	</style>
</head>

<body>

    <div class="row">
        <div class="col-md-2"></div>
        <div class="col-md-8">
            <div class="page-header center">
                <h1>Part 1: <small>Your Problem</small></h1>
				<h2><small>agale & ahojsak </small></h2>
            </div>
			<h2>Problem</h2>
			<p>
				Yelp receives thousands of reviews each day, and it must decide which reviews to highlight on businesses pages. Identifying which reviews other users will find useful, cool, or funny, provides additional data which Yelp can use when deciding which reviews to surface on business pages.
			</p>
			<h2>Hypothesis</h2>
			<p>
				Reviews that have gotten similar vote types (useful, funny, cool, and combinations of the three) tend to use similar language and give similar ratings, so a Naive Bayes classifier will be able to accurately predict the vote types that a new review will receive.
			</p>
			<h2>Data</h2>
			<p>
				We are using the publicly available Yelp dataset. This dataset contains review information on businesses in the greater Phoenix, AZ metropolitan area.
				<br>
				We separate the dataset randomly into a training set and a test set. 30% of the reviews are put into the training set, and the other 70% are put into the test set.  Each set contains the text of each individual review, the number of stars the review gave the business, and the number of cool/useful/funny votes the review itself received.
				<br>
				The following graph shows the distribution of reviews based on the number of stars each review gave the business and the type of votes the review received.

				<h4>Distribution of combinations of votes by vote types</h4>
				<div class="chart" id="chart2"></div>
			</p>
			<h2>Methodology</h2>
			<p>
			<ul>
				<li>We first performed feature extraction on the text of each review using the Bag-of-words model. CountVectorizer was used with a customized tokenizer to encode the unigram feature vectors. The tokenizer strips the text of all punctuation and applies porter stemming to each word. We also tried removing stop words and using bigram features, but both of these modifications resulted in worse classifier accuracy.</li>
				<li>We then implemented a Multinomial Naive Bayes classifier that predicts whether a review will be labeled as useful, funny, or cool, or some combination of the vote types. A review is defined as funny, cool, etc. if the review received at least one vote for the specific category. 
				<br>
				We experimented with both the type of classifier and the size of the buckets. The Multinomial Naive Bayes classifier had better performance than the Binomial Naive Bayes, Gaussian Naive Bayes, Logisitc Regression, and SVM classifiers. We also experimented with having buckets of reviews with no votes, 1-9 votes, and 10+ votes for each category, as well as 0,1-2,3-4,5+ votes, and many other combinations. Ultimately, it was determined that having only two buckets - one for no votes and the other for 1+ votes - resulted in the most accurate predictions.  </li>
				
			</ul>
			</p>
			<h2>Results</h2>
			<p>
	         	<h3>Training and Test Accuracy</h3>
	         	<p>
	         		The classifier clearly predicted funny reviews most accurately with 75.03% test accuracy. Cool and useful votes were much harder to predict with 69.62% and 61.88% test accuracies respectively. This is expected, as language can be humorous, but coolness and usefulness are more subjective traits and depend more on the overall message of the text and less on the actual language used.
	         	</p>
				<div class="chart" id="chart6"></div>
				<h3>Average stars for each vote type</h3>
				<p>
					It appears to be that reviews that people find 'cool' tend to be more positive towards the business being reviewed, while people think that, on average, more negative reviews are funnier.
				</p>
				<div class="chart" id="chart4"></div>


				<h3>Ratio of stars for each vote type</h3>
				<p>
					Reviews that were both funny and useful contained the largest percentage of one star ratings of any combination, with almost half of the reviews giving a rating of only one or two stars. On the other hand, almost half of reviews marked as cool gave a rating of five stars and ~80% of them were four or five star reviews.
				</p>
				<div class="chart" id="chart3"></div>
				<h3>Word cloud for features correlated with vote types</h3>
				<div class="chart" id="chart5"></div>
			</p>

        <div class="col-md-2"></div>
    </div>

    <script src="http://d3js.org/d3.v3.min.js"></script>
	<script src="js/chart1.js" type="text/javascript"></script>
	<script src="js/chart2.js" type="text/javascript"></script>
	<script src="js/chart3.js" type="text/javascript"></script>
	<script src="js/chart4.js" type="text/javascript"></script>
	<script src="js/chart5.js" type="text/javascript"></script>
	<script src="js/chart6.js" type="text/javascript"></script>
</body>
</html>

